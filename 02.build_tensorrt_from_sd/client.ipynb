{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import (\n",
    "    DDIMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    PNDMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    ")\n",
    "\n",
    "import json\n",
    "from optimum.pipelines.diffusers.pipeline_stable_diffusion import (\n",
    "    StableDiffusionPipelineMixin,\n",
    ")\n",
    "from optimum.pipelines.diffusers.pipeline_utils import rescale_noise_cfg\n",
    "from typing import Callable, List, Optional, Union\n",
    "import inspect\n",
    "import numpy as np\n",
    "import torch\n",
    "from abc import abstractmethod\n",
    "from tritonclient.grpc import InferenceServerClient, InferInput, InferRequestedOutput\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "from typing import Optional, List\n",
    "import numpy as np\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "\n",
    "from optimum.modeling_base import OptimizedModel\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "from diffusers import (\n",
    "    DDIMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    PNDMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    ")\n",
    "from transformers import CLIPFeatureExtractor, CLIPTokenizer\n",
    "\n",
    "from optimum.utils import (\n",
    "    DIFFUSION_MODEL_TEXT_ENCODER_2_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_UNET_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER,\n",
    "    DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER,\n",
    ")\n",
    "from optimum.pipelines.diffusers.pipeline_utils import VaeImageProcessor\n",
    "\n",
    "from optimum.exporters import TasksManager\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    GenerationMixin,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Triton Model parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _RemoteTritonDiffusionModelPart:\n",
    "    def __init__(\n",
    "        self, channel: str, model_name: str, model_version: Optional[int] = None\n",
    "    ):\n",
    "        self._client: InferenceServerClient = InferenceServerClient(channel)\n",
    "        self._model_name: str = model_name\n",
    "        self._model_version: str = model_version if model_version is not None else \"\"\n",
    "\n",
    "        if not self._client.is_server_live():\n",
    "            raise ConnectionError(\"Triton server is not live\")\n",
    "\n",
    "        if not self._client.is_server_ready():\n",
    "            raise ConnectionError(\"Triton server is not ready\")\n",
    "\n",
    "        if not self._client.is_model_ready(self._model_name, self._model_version):\n",
    "            raise ConnectionError(f\"Model {self._model_name} is not ready\")\n",
    "\n",
    "        self._metadata = self._client.get_model_metadata(\n",
    "            self._model_name, self._model_version\n",
    "        )\n",
    "\n",
    "        self._input_dtypes = {\n",
    "            input_.name: input_.datatype for input_ in self._metadata.inputs\n",
    "        }\n",
    "\n",
    "        self.output_names = [output_.name for output_ in self._metadata.outputs]\n",
    "\n",
    "    def make_infer_input(self, input_name: str, input_data: np.ndarray) -> InferInput:\n",
    "        if input_name not in self._input_dtypes.keys():\n",
    "            raise ValueError(f\"Input '{input_name}' is not found in the model\")\n",
    "\n",
    "        expected_data_type = self._input_dtypes[input_name]\n",
    "\n",
    "        actual_data_type = np_to_triton_dtype(input_data.dtype)\n",
    "\n",
    "        if actual_data_type != expected_data_type:\n",
    "            raise ValueError(\n",
    "                f\"Input '{input_name}' has dtype '{actual_data_type}' but expected '{expected_data_type}'\"\n",
    "            )\n",
    "\n",
    "        infer_input = InferInput(input_name, input_data.shape, actual_data_type)\n",
    "        infer_input.set_data_from_numpy(input_data)\n",
    "\n",
    "        return infer_input\n",
    "\n",
    "    def make_infer_requested_outputs(\n",
    "        self,\n",
    "    ) -> List[InferRequestedOutput]:\n",
    "        return [InferRequestedOutput(name) for name in self.output_names]\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "\n",
    "class RemoteTritonTextEncoder(_RemoteTritonDiffusionModelPart):\n",
    "    def forward(self, input_ids: np.ndarray):\n",
    "        results = self._client.infer(\n",
    "            model_name=self._model_name,\n",
    "            inputs=[self.make_infer_input(\"input_ids\", input_ids)],\n",
    "            outputs=self.make_infer_requested_outputs(),\n",
    "        )\n",
    "        return [results.as_numpy(\"last_hidden_state\")]\n",
    "\n",
    "\n",
    "class RemoteTritonModelUnet(_RemoteTritonDiffusionModelPart):\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: np.ndarray,\n",
    "        timestep: np.ndarray,\n",
    "        encoder_hidden_states: np.ndarray,\n",
    "        text_embeds: Optional[np.ndarray] = None,\n",
    "        time_ids: Optional[np.ndarray] = None,\n",
    "        timestep_cond: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        inputs = [\n",
    "            self.make_infer_input(\"sample\", sample),\n",
    "            self.make_infer_input(\"timestep\", timestep),\n",
    "            self.make_infer_input(\"encoder_hidden_states\", encoder_hidden_states),\n",
    "        ]\n",
    "\n",
    "        if text_embeds is not None:\n",
    "            inputs.append(self.make_infer_input(\"text_embeds\", text_embeds))\n",
    "        if time_ids is not None:\n",
    "            inputs.append(self.make_infer_input(\"time_ids\", time_ids))\n",
    "        if timestep_cond is not None:\n",
    "            inputs.append(self.make_infer_input(\"timestep_cond\", timestep_cond))\n",
    "\n",
    "        results = self._client.infer(\n",
    "            model_name=self._model_name,\n",
    "            inputs=inputs,\n",
    "            outputs=self.make_infer_requested_outputs(),\n",
    "        )\n",
    "        return [results.as_numpy(name) for name in self.output_names]\n",
    "\n",
    "\n",
    "class RemoteTritonModelVaeDecoder(_RemoteTritonDiffusionModelPart):\n",
    "    def forward(self, latent_sample: np.ndarray):\n",
    "        results = self._client.infer(\n",
    "            model_name=self._model_name,\n",
    "            inputs=[self.make_infer_input(\"latent_sample\", latent_sample)],\n",
    "            outputs=self.make_infer_requested_outputs(),\n",
    "        )\n",
    "        return [results.as_numpy(name) for name in self.output_names]\n",
    "\n",
    "\n",
    "class RemoteTritonModelVaeEncoder(_RemoteTritonDiffusionModelPart):\n",
    "    def forward(self, sample: np.ndarray):\n",
    "        results = self._client.infer(\n",
    "            model_name=self._model_name,\n",
    "            inputs=[self.make_infer_input(\"sample\", sample)],\n",
    "            outputs=self.make_infer_requested_outputs(),\n",
    "        )\n",
    "        return [results.as_numpy(name) for name in self.output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = RemoteTritonTextEncoder(\"localhost:8001\", \"text_encoder\")\n",
    "with open(\"./exported-models/onnx/stable-diffusion-v1-5/text_encoder/config.json\") as f:\n",
    "    text_encoder.config = json.load(f)\n",
    "\n",
    "unet = RemoteTritonModelUnet(\"localhost:8001\", \"unet\")\n",
    "with open(\"./exported-models/onnx/stable-diffusion-v1-5/unet/config.json\") as f:\n",
    "    unet.config = json.load(f)\n",
    "\n",
    "vae_decoder = RemoteTritonModelVaeDecoder(\"localhost:8001\", \"vae_decoder\")\n",
    "with open(\"./exported-models/onnx/stable-diffusion-v1-5/vae_decoder/config.json\") as f:\n",
    "    vae_decoder.config = json.load(f)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./exported-models/onnx/stable-diffusion-v1-5/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedStableDiffusionPipelineMixin(StableDiffusionPipelineMixin):\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: int = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[np.random.RandomState] = None,\n",
    "        latents: Optional[np.ndarray] = None,\n",
    "        prompt_embeds: Optional[np.ndarray] = None,\n",
    "        negative_prompt_embeds: Optional[np.ndarray] = None,\n",
    "        output_type: str = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        callback: Optional[Callable[[int, int, np.ndarray], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        guidance_rescale: float = 0.0,\n",
    "    ):\n",
    "        height = (\n",
    "            height or self.unet.config.get(\"sample_size\", 64) * self.vae_scale_factor\n",
    "        )\n",
    "        width = width or self.unet.config.get(\"sample_size\", 64) * self.vae_scale_factor\n",
    "\n",
    "        # check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "        )\n",
    "\n",
    "        # define call parameters\n",
    "        if isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        if generator is None:\n",
    "            generator = np.random\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        prompt_embeds = self._encode_prompt(\n",
    "            prompt,\n",
    "            num_images_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "        )\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            self.unet.config.get(\"in_channels\", 4),\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "        accepts_eta = \"eta\" in set(\n",
    "            inspect.signature(self.scheduler.step).parameters.keys()\n",
    "        )\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        # Adapted from diffusers to extend it for other runtimes than ORT\n",
    "        timestep_dtype = self.unet.input_dtype.get(\"timestep\", np.float32)\n",
    "\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        for i, t in enumerate(self.progress_bar(timesteps)):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = (\n",
    "                np.concatenate([latents] * 2)\n",
    "                if do_classifier_free_guidance\n",
    "                else latents\n",
    "            )\n",
    "            latent_model_input = self.scheduler.scale_model_input(\n",
    "                torch.from_numpy(latent_model_input), t\n",
    "            )\n",
    "            latent_model_input = latent_model_input.cpu().numpy()\n",
    "            # predict the noise residual\n",
    "\n",
    "            timestep = np.array([t], dtype=timestep_dtype).reshape(1, 1).repeat(\n",
    "                batch_size * 2, 0\n",
    "            )\n",
    "            print(latent_model_input.shape, timestep.shape, prompt_embeds.shape)\n",
    "            noise_pred = self.unet(\n",
    "                sample=latent_model_input,\n",
    "                timestep=timestep,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "            )\n",
    "            noise_pred = noise_pred[0]\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = np.split(noise_pred, 2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "                    noise_pred_text - noise_pred_uncond\n",
    "                )\n",
    "                if guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n",
    "                    noise_pred = rescale_noise_cfg(\n",
    "                        noise_pred, noise_pred_text, guidance_rescale=guidance_rescale\n",
    "                    )\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            scheduler_output = self.scheduler.step(\n",
    "                torch.from_numpy(noise_pred),\n",
    "                t,\n",
    "                torch.from_numpy(latents),\n",
    "                **extra_step_kwargs\n",
    "            )\n",
    "            latents = scheduler_output.prev_sample.numpy()\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if i == len(timesteps) - 1 or (\n",
    "                (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n",
    "            ):\n",
    "                if callback is not None and i % callback_steps == 0:\n",
    "                    callback(i, t, latents)\n",
    "\n",
    "        if output_type == \"latent\":\n",
    "            image = latents\n",
    "            has_nsfw_concept = None\n",
    "        else:\n",
    "            latents /= self.vae_decoder.config.get(\"scaling_factor\", 0.18215)\n",
    "\n",
    "            image = np.concatenate(\n",
    "                [\n",
    "                    self.vae_decoder(latent_sample=latents[i : i + 1])[0]\n",
    "                    for i in range(latents.shape[0])\n",
    "                ]\n",
    "            )\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image)\n",
    "\n",
    "        if has_nsfw_concept is None:\n",
    "            do_denormalize = [True] * image.shape[0]\n",
    "        else:\n",
    "            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n",
    "\n",
    "        image = self.image_processor.postprocess(\n",
    "            image, output_type=output_type, do_denormalize=do_denormalize\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(\n",
    "            images=image, nsfw_content_detected=has_nsfw_concept\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Remote triton pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteTritonModel(OptimizedModel):\n",
    "    model_type = \"remote_triton_model\"\n",
    "    auto_model_class = AutoModel\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _auto_model_to_task(cls, auto_model_class):\n",
    "        \"\"\"\n",
    "        Get the task corresponding to a class (for example AutoModelForXXX in transformers).\n",
    "        \"\"\"\n",
    "        return TasksManager.infer_task_from_model(auto_model_class)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        config: \"PretrainedConfig\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(model, config)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def can_generate(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns whether this model can generate sequences with `.generate()`.\n",
    "        \"\"\"\n",
    "        return isinstance(self, GenerationMixin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteTritonStableDiffusionPipelineBase(RemoteTritonModel):\n",
    "    auto_model_class = StableDiffusionPipeline\n",
    "    main_input_name = \"input_ids\"\n",
    "    base_model_prefix = \"onnx_model\"\n",
    "    config_name = \"model_index.json\"\n",
    "    sub_component_config_name = \"config.json\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae_decoder: RemoteTritonModelVaeDecoder,\n",
    "        text_encoder: RemoteTritonTextEncoder,\n",
    "        unet: RemoteTritonModelUnet,\n",
    "        config: Dict[str, Any],\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
    "        feature_extractor: Optional[CLIPFeatureExtractor] = None,\n",
    "        vae_encoder: Optional[RemoteTritonModelVaeEncoder] = None,\n",
    "        text_encoder_2: Optional[RemoteTritonTextEncoder] = None,\n",
    "        tokenizer_2: Optional[CLIPTokenizer] = None,\n",
    "    ):\n",
    "        self._internal_dict = config\n",
    "        self.vae_decoder = vae_decoder\n",
    "        self.unet = unet\n",
    "\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vae_encoder = vae_encoder\n",
    "        self.text_encoder_2 = text_encoder_2\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        self.scheduler = scheduler\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.safety_checker = None\n",
    "\n",
    "        sub_models = {\n",
    "            DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER: self.text_encoder,\n",
    "            DIFFUSION_MODEL_UNET_SUBFOLDER: self.unet,\n",
    "            DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER: self.vae_decoder,\n",
    "            DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER: self.vae_encoder,\n",
    "            DIFFUSION_MODEL_TEXT_ENCODER_2_SUBFOLDER: self.text_encoder_2,\n",
    "        }\n",
    "\n",
    "        for name in sub_models.keys():\n",
    "            self._internal_dict[name] = (\n",
    "                (\"diffusers\", \"OnnxRuntimeModel\")\n",
    "                if sub_models[name] is not None\n",
    "                else (None, None)\n",
    "            )\n",
    "        self._internal_dict.pop(\"vae\", None)\n",
    "\n",
    "        if \"block_out_channels\" in self.vae_decoder.config:\n",
    "            self.vae_scale_factor = 2 ** (\n",
    "                len(self.vae_decoder.config[\"block_out_channels\"]) - 1\n",
    "            )\n",
    "        else:\n",
    "            self.vae_scale_factor = 8\n",
    "\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model():\n",
    "        text_encoder = RemoteTritonTextEncoder(\"localhost:8001\", \"text_encoder\")\n",
    "        with open(\n",
    "            \"./exported-models/onnx/stable-diffusion-v1-5/text_encoder/config.json\"\n",
    "        ) as f:\n",
    "            text_encoder.config = json.load(f)\n",
    "\n",
    "        unet = RemoteTritonModelUnet(\"localhost:8001\", \"unet\")\n",
    "        with open(\"./exported-models/onnx/stable-diffusion-v1-5/unet/config.json\") as f:\n",
    "            unet.config = json.load(f)\n",
    "        unet.input_dtype = {\"timestep\": np.dtype(np.int32)}\n",
    "\n",
    "\n",
    "        vae_decoder = RemoteTritonModelVaeDecoder(\"localhost:8001\", \"vae_decoder\")\n",
    "        with open(\n",
    "            \"./exported-models/onnx/stable-diffusion-v1-5/vae_decoder/config.json\"\n",
    "        ) as f:\n",
    "            vae_decoder.config = json.load(f)\n",
    "\n",
    "        return vae_decoder, text_encoder, unet, None, None\n",
    "\n",
    "    def _save_pretrained(self, save_directory: Union[str, Path]):\n",
    "        # save_directory = Path(save_directory)\n",
    "        # src_to_dst_path = {\n",
    "        #     self.vae_decoder_model_path: save_directory / DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER / ONNX_WEIGHTS_NAME,\n",
    "        #     self.text_encoder_model_path: save_directory / DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER / ONNX_WEIGHTS_NAME,\n",
    "        #     self.unet_model_path: save_directory / DIFFUSION_MODEL_UNET_SUBFOLDER / ONNX_WEIGHTS_NAME,\n",
    "        # }\n",
    "\n",
    "        # sub_models_to_save = {\n",
    "        #     self.vae_encoder_model_path: DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER,\n",
    "        #     self.text_encoder_2_model_path: DIFFUSION_MODEL_TEXT_ENCODER_2_SUBFOLDER,\n",
    "        # }\n",
    "        # for path, subfolder in sub_models_to_save.items():\n",
    "        #     if path is not None:\n",
    "        #         src_to_dst_path[path] = save_directory / subfolder / ONNX_WEIGHTS_NAME\n",
    "\n",
    "        # # TODO: Modify _get_external_data_paths to give dictionnary\n",
    "        # src_paths = list(src_to_dst_path.keys())\n",
    "        # dst_paths = list(src_to_dst_path.values())\n",
    "        # # Add external data paths in case of large models\n",
    "        # src_paths, dst_paths = _get_external_data_paths(src_paths, dst_paths)\n",
    "\n",
    "        # for src_path, dst_path in zip(src_paths, dst_paths):\n",
    "        #     dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        #     shutil.copyfile(src_path, dst_path)\n",
    "        #     config_path = src_path.parent / self.sub_component_config_name\n",
    "        #     if config_path.is_file():\n",
    "        #         shutil.copyfile(config_path, dst_path.parent / self.sub_component_config_name)\n",
    "\n",
    "        # self.scheduler.save_pretrained(save_directory / \"scheduler\")\n",
    "\n",
    "        # if self.feature_extractor is not None:\n",
    "        #     self.feature_extractor.save_pretrained(save_directory / \"feature_extractor\")\n",
    "        # if self.tokenizer is not None:\n",
    "        #     self.tokenizer.save_pretrained(save_directory / \"tokenizer\")\n",
    "        # if self.tokenizer_2 is not None:\n",
    "        #     self.tokenizer_2.save_pretrained(save_directory / \"tokenizer_2\")\n",
    "\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def _from_pretrained(\n",
    "        cls,\n",
    "        model_id: Union[str, Path],\n",
    "        config: Dict[str, Any],\n",
    "        **kwargs,\n",
    "    ):        \n",
    "        model_id = str(model_id)\n",
    "        patterns = set(config.keys())\n",
    "        sub_models_to_load = patterns.intersection({\"feature_extractor\", \"tokenizer\", \"tokenizer_2\", \"scheduler\"})\n",
    "\n",
    "        if not os.path.isdir(model_id):\n",
    "            raise ValueError(f\"Model {model_id} is not a directory\")\n",
    "        new_model_save_dir = Path(model_id)\n",
    "\n",
    "        sub_models = {}\n",
    "        for name in sub_models_to_load:\n",
    "            library_name, library_classes = config[name]\n",
    "            if library_classes is not None:\n",
    "                library = importlib.import_module(library_name)\n",
    "                class_obj = getattr(library, library_classes)\n",
    "\n",
    "                load_method = getattr(class_obj, \"from_pretrained\")\n",
    "\n",
    "                if (new_model_save_dir / name).is_dir():\n",
    "                    sub_models[name] = load_method(new_model_save_dir / name)\n",
    "                else:\n",
    "                    sub_models[name] = load_method(new_model_save_dir)\n",
    "\n",
    "        vae_decoder, text_encoder, unet, vae_encoder, text_encoder_2 = cls.load_model()\n",
    "\n",
    "        return cls(\n",
    "            vae_decoder=vae_decoder,\n",
    "            text_encoder=text_encoder,\n",
    "            unet=unet,\n",
    "            config=config,\n",
    "            tokenizer=sub_models.get(\"tokenizer\", None),\n",
    "            scheduler=sub_models.get(\"scheduler\"),\n",
    "            feature_extractor=sub_models.get(\"feature_extractor\", None),\n",
    "            tokenizer_2=sub_models.get(\"tokenizer_2\", None),\n",
    "            vae_encoder=vae_encoder,\n",
    "            text_encoder_2=text_encoder_2,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _load_config(cls, config_name_or_path: Union[str, os.PathLike], **kwargs):\n",
    "        return cls.load_config(config_name_or_path, **kwargs)\n",
    "\n",
    "    def _save_config(self, save_directory):\n",
    "        self.save_config(save_directory)\n",
    "\n",
    "\n",
    "class RemoteTritonStableDiffusionPipeline(\n",
    "    RemoteTritonStableDiffusionPipelineBase, ModifiedStableDiffusionPipelineMixin\n",
    "):\n",
    "    __call__ = ModifiedStableDiffusionPipelineMixin.__call__\n",
    "\n",
    "\n",
    "pipeline = RemoteTritonStableDiffusionPipeline.from_pretrained(\n",
    "    \"./exported-models/onnx/stable-diffusion-v1-5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 64, 64) (2, 1) (2, 77, 768)\n",
      "(2, 4, 64, 64) (2, 1) (2, 77, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 64, 64) (2, 1) (2, 77, 768)\n"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.INTERNAL] request specifies invalid shape for input 'latent_sample' for vae_decoder_0. Error details: model expected the shape of dimension 0 to be between 2 and 2 but received 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msailing ship in storm by Leonardo da Vinci\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[13], line 141\u001b[0m, in \u001b[0;36mModifiedStableDiffusionPipelineMixin.__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps, guidance_rescale)\u001b[0m\n\u001b[1;32m    138\u001b[0m     latents \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_decoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.18215\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# it seems likes there is a strange result for using half-precision vae decoder if batchsize>1\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 141\u001b[0m         [\n\u001b[1;32m    142\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_decoder(latent_sample\u001b[38;5;241m=\u001b[39mlatents[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    143\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    144\u001b[0m         ]\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m     image, has_nsfw_concept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_safety_checker(image)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_nsfw_concept \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[13], line 142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    138\u001b[0m     latents \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_decoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.18215\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# it seems likes there is a strange result for using half-precision vae decoder if batchsize>1\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m    141\u001b[0m         [\n\u001b[0;32m--> 142\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    143\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    144\u001b[0m         ]\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m     image, has_nsfw_concept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_safety_checker(image)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_nsfw_concept \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m, in \u001b[0;36m_RemoteTritonDiffusionModelPart.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 102\u001b[0m, in \u001b[0;36mRemoteTritonModelVaeDecoder.forward\u001b[0;34m(self, latent_sample)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, latent_sample: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 102\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_infer_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatent_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_infer_requested_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [results\u001b[38;5;241m.\u001b[39mas_numpy(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_names]\n",
      "File \u001b[0;32m~/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/tritonclient/grpc/_client.py:1572\u001b[0m, in \u001b[0;36mInferenceServerClient.infer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers, compression_algorithm, parameters)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[0;32m-> 1572\u001b[0m     \u001b[43mraise_error_grpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpc_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/tritonclient/grpc/_utils.py:77\u001b[0m, in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error_grpc\u001b[39m(rpc_error):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise an :py:class:`InferenceServerException` from a gRPC error.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    InferenceServerException\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_error_grpc(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.INTERNAL] request specifies invalid shape for input 'latent_sample' for vae_decoder_0. Error details: model expected the shape of dimension 0 to be between 2 and 2 but received 1"
     ]
    }
   ],
   "source": [
    "prompt = \"sailing ship in storm by Leonardo da Vinci\"\n",
    "image = pipeline(prompt, num_inference_steps=2, num_images_per_prompt=2, guidance_scale=0).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1], dtype=np.int32).reshape(1, 1).repeat(2, 0).shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton-huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
